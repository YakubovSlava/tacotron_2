{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import TacotronPreprocessor, TTSDataset, collate_fn, reconstruct_audio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio import transforms\n",
    "from torchaudio.functional import preemphasis\n",
    "import hyperparams as hps\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderConvLayer(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size) -> None:\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size, bias=False, padding=2, dilation=1),\n",
    "            nn.BatchNorm1d(output_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, characters_num, embedding_size, lstm_hidden_size) -> None:\n",
    "        super().__init__()\n",
    "        self.char_embedding = nn.Embedding(characters_num, embedding_size)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            EncoderConvLayer(embedding_size, embedding_size, 5),\n",
    "            EncoderConvLayer(embedding_size, embedding_size, 5),\n",
    "            EncoderConvLayer(embedding_size, embedding_size, 5),\n",
    "        )\n",
    "        self.rnn = nn.LSTM(input_size=embedding_size,\n",
    "                           hidden_size=lstm_hidden_size,\n",
    "                           bidirectional=True, batch_first=True)\n",
    "        self.rnn_dropout = nn.Dropout(0.1)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.tensor, mask_idx=None):\n",
    "        \"\"\"\n",
    "        На вход подается последовательность символов. Размерность [BATCH_SIZE, NUM_CHARACTERS]\n",
    "        \"\"\"\n",
    "        x = self.char_embedding(x)  #[BATCH_SIZE, NUM_CHARACTERS, EMB_SIZE]\n",
    "        x = x.transpose(1,2) #[BATCH_SIZE, EMB_SIZE, NUM_CHARACTERS]\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.transpose(1,2) #[BATCH_SIZE, NUM_CHARACTERS, CONV_EMB]\n",
    "\n",
    "        if mask_idx is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, mask_idx, batch_first=True, enforce_sorted=False)   \n",
    "        x = self.rnn(x)[0]\n",
    "        if mask_idx is not None:   \n",
    "            x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        x = self.rnn_dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNet(nn.Module):\n",
    "    def __init__(self, num_mels, prenet_hidden_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Linear(num_mels, prenet_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(prenet_hidden_dim, prenet_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.module(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Самая сложная часть модели\n",
    "class Tacotron2Attention(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.location = nn.Conv1d(in_channels=1, out_channels=hps.ATTENTION_LOCATION_FILTERS, \n",
    "                                  kernel_size=hps.ATTENTION_LOCATION_KERNEL_SIZE, dilation=1, padding=((hps.ATTENTION_LOCATION_KERNEL_SIZE-1)//2), bias=False)\n",
    "        self.location_linear = nn.Linear(hps.ATTENTION_LOCATION_FILTERS, hps.ATTENTION_DIM, bias=False)\n",
    "        self.rnn_hs_linear = nn.Linear(hps.DECODER_RNN_HIDDEN_DIM * 2, hps.ATTENTION_DIM, bias=False)\n",
    "\n",
    "        self.alignments_linear = nn.Linear(hps.ATTENTION_DIM, 1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, rnn_hs, encoder_output, processed_encoder_output, location_attention, mask=None):\n",
    "        \"\"\"\n",
    "         На вход подсчета аттеншна передаются:\n",
    "            - Накопленная информация из РНН \n",
    "            - Выход энкодера (BS, seq_len, 2*encoder_lstm_dim)\n",
    "            - Прогнанный через линейный слой энкодер (чтобы не дублить операцию)\n",
    "            - commulative-attention (BS, 1, seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        ### Location attention\n",
    "        #   Логика:\n",
    "        #   Входящий вектор на каждую букву прогоняем через свертку таким образом, чтобы у нас на каждую букву было 32 значения, которые будут говорить о том, что данная буква уже встретилась в выданном аудио\n",
    "        location_attention = location_attention.transpose(1,2) # [BS, seq_len, 1]\n",
    "        location_attention = self.location(location_attention) # [BS, seq_len, Location_attention_filters]\n",
    "        location_attention = self.dropout(location_attention)\n",
    "        location_attention = location_attention.transpose(1,2) # [BS, seq_len, Location_attention_filters]\n",
    "        location_attention = self.location_linear(location_attention) # [BS, seq_len, attention_dim]\n",
    "\n",
    "        rnn_hs = torch.cat((rnn_hs[0], rnn_hs[1]), 1) # [BS, 2 * DECODER_RNN_HIDDEN_DIM]\n",
    "        rnn_hs = rnn_hs.unsqueeze(1) # [BS, 1, 2 * DECODER_RNN_HIDDEN_DIM]\n",
    "        rnn_hs = self.rnn_hs_linear(rnn_hs) # [BS, 1, Attention_dim]\n",
    "        alignments = nn.functional.tanh(rnn_hs + location_attention + processed_encoder_output) # (BS, seq_len, Attention_dim)\n",
    "        alignments = self.alignments_linear(alignments) # (BS, seq_len, 1)\n",
    "\n",
    "\n",
    "        if mask is not None:\n",
    "            alignments.data.masked_fill_(~mask.unsqueeze(-1), -torch.inf)\n",
    "        alignments = self.softmax(alignments) # (BS, seq_len, 1)\n",
    "\n",
    "        attention_score = (encoder_output.transpose(1, 2) @ alignments).transpose(1, 2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        return  attention_score, alignments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderPostNetConv(nn.Module):\n",
    "    def __init__(self, in_kernels, out_kernels, last_layer=False) -> None:\n",
    "        super().__init__()\n",
    "        self.last_layer = last_layer\n",
    "        self.post_net = nn.Sequential(\n",
    "            nn.Conv1d(in_kernels, out_kernels, hps.POSTNET_KERNEL_SIZE, padding=2, bias=False, dilation=1),\n",
    "            nn.BatchNorm1d(hps.POSTNET_NUM_FILTERS),\n",
    "            nn.Identity() if last_layer else nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.post_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.prenet = PreNet(hps.N_MEL_FILTERBANKS, hps.PRENET_HIDDEN_SIZE)\n",
    "        self.encoder_linear = nn.Linear(hps.LSTM_HIDDEN_SIZE * 2, hps.ATTENTION_DIM)\n",
    "        self.decoder_rnn = nn.LSTM(input_size=hps.PRENET_HIDDEN_SIZE + hps.CHARACTER_EMB_SIZE, \n",
    "                                   hidden_size=hps.DECODER_RNN_HIDDEN_DIM, batch_first=True, num_layers=2)\n",
    "        self.attention = Tacotron2Attention()\n",
    "        self.linear_projection = nn.Linear(hps.DECODER_RNN_HIDDEN_DIM + hps.CHARACTER_EMB_SIZE, hps.N_MEL_FILTERBANKS)\n",
    "        self.stop_projection = nn.Linear(hps.DECODER_RNN_HIDDEN_DIM + hps.CHARACTER_EMB_SIZE, 1)\n",
    "\n",
    "        self.post_net = nn.Sequential(\n",
    "            DecoderPostNetConv(hps.N_MEL_FILTERBANKS, hps.POSTNET_NUM_FILTERS, hps.POSTNET_NUM_FILTERS),\n",
    "            DecoderPostNetConv(hps.POSTNET_NUM_FILTERS, hps.POSTNET_NUM_FILTERS),\n",
    "            DecoderPostNetConv(hps.POSTNET_NUM_FILTERS, hps.POSTNET_NUM_FILTERS),\n",
    "            DecoderPostNetConv(hps.POSTNET_NUM_FILTERS, hps.POSTNET_NUM_FILTERS),\n",
    "            DecoderPostNetConv(hps.POSTNET_NUM_FILTERS, hps.POSTNET_NUM_FILTERS, last_layer=True)\n",
    "        )\n",
    "        self.post_linear = nn.Linear(hps.POSTNET_NUM_FILTERS, hps.N_MEL_FILTERBANKS)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, mels, encoder_output, mask):\n",
    "        mels = mels.transpose(1,2)\n",
    "        mels = self.prenet(mels)\n",
    "        processed_encoder = self.encoder_linear(encoder_output)\n",
    "\n",
    "\n",
    "        next_h = torch.zeros(2, mels.shape[0], hps.DECODER_RNN_HIDDEN_DIM, device=mels.device)\n",
    "        next_c = torch.zeros(2, mels.shape[0], hps.DECODER_RNN_HIDDEN_DIM, device=mels.device)\n",
    "\n",
    "        mel_predictions = []\n",
    "        stop_tokens = []\n",
    "        curr_attention_context = torch.zeros(encoder_output.shape[0], 1, hps.CHARACTER_EMB_SIZE, device=mels.device)\n",
    "        cummulated_attention = torch.zeros(encoder_output.shape[0], encoder_output.shape[1], 1, device=mels.device)\n",
    "\n",
    "        next_h.requires_grad = True\n",
    "        next_c.requires_grad = True\n",
    "        curr_attention_context.requires_grad = True\n",
    "        cummulated_attention.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(mels.shape[1]):\n",
    "            curr_mel = mels[:, i, :].unsqueeze(1)\n",
    "            curr_rnn_input = torch.cat((curr_mel, curr_attention_context), dim=-1)\n",
    "            next_mel, (next_h, next_c) = self.decoder_rnn(curr_rnn_input, (next_h, next_c))\n",
    "            next_mel = self.dropout(next_mel)\n",
    "            curr_attention_context, alignments = self.attention(next_h, encoder_output, processed_encoder, cummulated_attention, mask)\n",
    "            cummulated_attention = cummulated_attention + alignments\n",
    "            next_mel_inp = torch.cat((next_mel, curr_attention_context), dim=2)\n",
    "            next_mel = self.linear_projection(next_mel_inp)\n",
    "            next_stop = self.stop_projection(next_mel_inp).squeeze(1)\n",
    "            mel_predictions.append(next_mel)\n",
    "            stop_tokens.append(next_stop)\n",
    "\n",
    "        result_mel = torch.cat(mel_predictions, dim=1)\n",
    "        result_stops = torch.cat(stop_tokens, dim=1)\n",
    "        result_mel = result_mel.transpose(1, 2)\n",
    "        result_mel_post = self.post_net(result_mel)\n",
    "        result_mel_post = result_mel_post.transpose(1,2)\n",
    "        result_mel_post = self.post_linear(result_mel_post)\n",
    "        result_mel_post = result_mel_post.transpose(1,2)\n",
    "        result_mel_post = result_mel + result_mel_post\n",
    "        \n",
    "        \n",
    "        return result_mel, result_mel_post, result_stops\n",
    "    \n",
    "    def predict(self, encoder_output):\n",
    "        mels = torch.log(torch.clamp(torch.zeros(encoder_output.shape[0], 1, hps.N_MEL_FILTERBANKS, device=encoder_output.device), hps.CLIPMIN))\n",
    "        mels = self.prenet(mels)\n",
    "        processed_encoder = self.encoder_linear(encoder_output)\n",
    "\n",
    "\n",
    "        next_h = torch.zeros(2, mels.shape[0], hps.DECODER_RNN_HIDDEN_DIM, device=mels.device)\n",
    "        next_c = torch.zeros(2, mels.shape[0], hps.DECODER_RNN_HIDDEN_DIM, device=mels.device)\n",
    "\n",
    "        mel_predictions = []\n",
    "        mel_predictions_post = []\n",
    "        stop_tokens = []\n",
    "\n",
    "        curr_attention_context = torch.zeros(encoder_output.shape[0], 1, hps.CHARACTER_EMB_SIZE, device=mels.device)\n",
    "        cummulated_attention = torch.zeros(encoder_output.shape[0], encoder_output.shape[1], 1, device=mels.device)\n",
    "\n",
    "        next_h.requires_grad = True\n",
    "        next_c.requires_grad = True\n",
    "        curr_attention_context.requires_grad = True\n",
    "        cummulated_attention.requires_grad = True\n",
    "\n",
    "\n",
    "        for i in range(1500):\n",
    "            curr_mel = mels\n",
    "            curr_rnn_input = torch.cat((curr_mel, curr_attention_context), dim=-1)\n",
    "            next_mel, (next_h, next_c) = self.decoder_rnn(curr_rnn_input, (next_h, next_c))\n",
    "            curr_attention_context, alignments = self.attention(next_h, encoder_output, processed_encoder, cummulated_attention)\n",
    "            cummulated_attention = cummulated_attention + alignments\n",
    "            next_mel_inp = torch.cat((next_mel, curr_attention_context), dim=2)\n",
    "            next_mel = self.linear_projection(next_mel_inp)\n",
    "\n",
    "            next_mel_post = self.post_net(next_mel.transpose(1,2)).transpose(1,2)\n",
    "            next_mel_post = self.post_linear(next_mel_post)\n",
    "            next_mel_post = next_mel_post + next_mel\n",
    "            mels = self.prenet(next_mel_post)\n",
    "\n",
    "            next_stop = self.stop_projection(next_mel_inp).squeeze(1)\n",
    "            mel_predictions.append(next_mel)\n",
    "            mel_predictions_post.append(next_mel_post)\n",
    "            stop_tokens.append(next_stop)\n",
    "\n",
    "        \n",
    "        result_mel = torch.cat(mel_predictions, dim=1)\n",
    "        result_mel_post = torch.cat(mel_predictions_post, dim=1)\n",
    "        result_stops = torch.cat(stop_tokens, dim=1)\n",
    "\n",
    "        return result_mel_post.transpose(1,2), result_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron2(nn.Module):\n",
    "    def __init__(self, characters_num: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.characters_num = characters_num\n",
    "        self.encoder = Encoder(characters_num, hps.CHARACTER_EMB_SIZE, hps.LSTM_HIDDEN_SIZE)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def get_mask(self, mask_idx):\n",
    "        mask = torch.zeros(mask_idx.shape[0], max(mask_idx))\n",
    "        mask = ((torch.arange(0, max(mask_idx)).unsqueeze(1)<torch.tensor(mask_idx))).transpose(0,1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, text, mels, mask_idx):\n",
    "        encoder_output = self.encoder(text, torch.tensor(mask_idx))\n",
    "        mask = self.get_mask(torch.tensor(mask_idx))\n",
    "        mask = mask.to(encoder_output.device)\n",
    "        decoder_output = self.decoder(mels, encoder_output, mask)\n",
    "        return decoder_output\n",
    "    \n",
    "    def predict(self, text):\n",
    "        encoder_output = self.encoder(text)\n",
    "        decoder_output = self.decoder.predict(encoder_output)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron2Loss(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "    def forward(self, mel_true, mel_pred, mel_pred_post, stops, stops_pred):\n",
    "        mel_loss = self.mse_loss(mel_pred, mel_true) + self.mse_loss(mel_pred_post, mel_true)\n",
    "        stop_loss = self.bce_loss(stops_pred, stops)\n",
    "        return mel_loss + stop_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Tacotron2:\n\tsize mismatch for decoder.prenet.module.0.weight: copying a param with shape torch.Size([256, 80]) from checkpoint, the shape in current model is torch.Size([256, 40]).\n\tsize mismatch for decoder.linear_projection.weight: copying a param with shape torch.Size([80, 1536]) from checkpoint, the shape in current model is torch.Size([40, 1536]).\n\tsize mismatch for decoder.linear_projection.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([40]).\n\tsize mismatch for decoder.post_net.0.post_net.0.weight: copying a param with shape torch.Size([512, 80, 5]) from checkpoint, the shape in current model is torch.Size([512, 40, 5]).\n\tsize mismatch for decoder.post_linear.weight: copying a param with shape torch.Size([80, 512]) from checkpoint, the shape in current model is torch.Size([40, 512]).\n\tsize mismatch for decoder.post_linear.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([40]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m loss \u001b[39m=\u001b[39m Tacotron2Loss()\n\u001b[0;32m     10\u001b[0m model \u001b[39m=\u001b[39m Tacotron2(vocab_size)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 11\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mmodel_saves\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mepoch_40_train_result\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     12\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m4e-4\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39m# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.5)\u001b[39;00m\n",
      "File \u001b[1;32md:\\mlp\\tacotron_2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Tacotron2:\n\tsize mismatch for decoder.prenet.module.0.weight: copying a param with shape torch.Size([256, 80]) from checkpoint, the shape in current model is torch.Size([256, 40]).\n\tsize mismatch for decoder.linear_projection.weight: copying a param with shape torch.Size([80, 1536]) from checkpoint, the shape in current model is torch.Size([40, 1536]).\n\tsize mismatch for decoder.linear_projection.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([40]).\n\tsize mismatch for decoder.post_net.0.post_net.0.weight: copying a param with shape torch.Size([512, 80, 5]) from checkpoint, the shape in current model is torch.Size([512, 40, 5]).\n\tsize mismatch for decoder.post_linear.weight: copying a param with shape torch.Size([80, 512]) from checkpoint, the shape in current model is torch.Size([40, 512]).\n\tsize mismatch for decoder.post_linear.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([40])."
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "dataset = TTSDataset()\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, collate_fn=collate_fn, shuffle=True,\n",
    "                         num_workers=8\n",
    "                         )\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_EPOCHS = 200\n",
    "vocab_size = dataset.preprocessor.vocab.shape[0] + 1\n",
    "loss = Tacotron2Loss()\n",
    "model = Tacotron2(vocab_size).to(DEVICE)\n",
    "# model.load_state_dict(torch.load('model_saves\\epoch_40_train_result'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 4e-4, weight_decay=1e-6)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.5)\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_text(text):\n",
    "    text = dataset.preprocessor.transform_single_text(text)\n",
    "    text = torch.tensor(text).unsqueeze(0)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yakub\\AppData\\Local\\Temp\\ipykernel_7988\\594783278.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for text, mel, stop, mask_idx in tqdm(dataloader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725d6d12e2654bcb92b74f380dfc5d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yakub\\AppData\\Local\\Temp\\ipykernel_7988\\3789512340.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = ((torch.arange(0, max(mask_idx)).unsqueeze(1)<torch.tensor(mask_idx))).transpose(0,1)\n",
      "C:\\Users\\yakub\\AppData\\Local\\Temp\\ipykernel_7988\\594783278.py:12: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(model.parameters(), 1.)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m loss_val\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     12\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m epoch_train_losses\u001b[39m.\u001b[39mappend(loss_val\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     15\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mLoss/train\u001b[39m\u001b[39m'\u001b[39m, loss_val, eval_step)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "eval_step = 0\n",
    "for epoch in range(40, NUM_EPOCHS):\n",
    "    model.train(True)\n",
    "    epoch_train_losses = [0]\n",
    "    for text, mel, stop, mask_idx in tqdm(dataloader):\n",
    "        text = text.to(DEVICE)\n",
    "        mel = mel.to(DEVICE)\n",
    "        stop = stop.to(DEVICE)\n",
    "        result_mel, result_mel_post, result_stops = model(text, mel, mask_idx)\n",
    "        loss_val = loss(mel[:, :, 1:], result_mel[:, :, :-1], result_mel_post[:, :, :-1], stop[:, 1:], result_stops[:, :-1])\n",
    "        loss_val.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), 1.)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append(loss_val.cpu().detach().numpy())\n",
    "        writer.add_scalar('Loss/train', loss_val, eval_step)\n",
    "        writer.add_scalar('LR/train', optimizer.param_groups[0]['lr'], eval_step)\n",
    "        eval_step += 1\n",
    "\n",
    "    model.train(False)\n",
    "    if epoch%5==0:\n",
    "        sample_text = code_text(\"Привет Это проверка генерации речи из текста!\").to(DEVICE)\n",
    "        audio, stops = model.predict(sample_text)\n",
    "        stops = nn.functional.sigmoid(stops)\n",
    "        stops = (stops>1).int()\n",
    "        stops[0] = 0\n",
    "        aud = reconstruct_audio(audio[0].detach(), stops[0].detach())\n",
    "        writer.add_audio(\"Model_results\", aud, sample_rate=hps.SAMPLE_RATE, global_step=epoch)\n",
    "        torch.save(model.state_dict(), f\"model_saves/epoch_{epoch}_train_result\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(408.8496, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sigmoid()(result_stops)[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(409., device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(False)\n",
    "sample_text = code_text(\"Привет Это проверка генерации речи из текста!\").to(DEVICE)\n",
    "audio, stops = model.predict(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nn\u001b[39m.\u001b[39;49mSigmoid()(stops)\u001b[39m.\u001b[39;49mlist()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'list'"
     ]
    }
   ],
   "source": [
    "nn.Sigmoid()(stops).list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 2]\n",
      "1 [1, 2]\n"
     ]
    }
   ],
   "source": [
    "for i, k in enumerate([[1,2], [1,2]]):\n",
    "    print(i, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 558])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.46365422396858"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100000/509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = dataset.preprocessor.vocab.shape[0] + 1\n",
    "model = Tacotron2(vocab_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_input_encoder = data[0].cuda()\n",
    "temp_input_decoder = data[1].cuda()\n",
    "with torch.no_grad():\n",
    "    decoder_output = model(temp_input_encoder, temp_input_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Tacotron2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-5.1191e-02, -5.5188e-02, -5.7950e-02,  ..., -5.7682e-02,\n",
       "           -5.3528e-02, -5.6456e-02],\n",
       "          [-1.6762e-02, -1.5057e-02, -1.3166e-02,  ..., -1.1898e-03,\n",
       "           -2.8852e-03, -1.2697e-03],\n",
       "          [ 2.6739e-03,  5.5839e-03,  7.8182e-03,  ...,  1.3650e-02,\n",
       "            9.1022e-03,  1.0750e-02],\n",
       "          ...,\n",
       "          [-5.3367e-02, -5.4048e-02, -5.3908e-02,  ..., -5.6514e-02,\n",
       "           -5.2911e-02, -5.1862e-02],\n",
       "          [ 4.7301e-02,  4.7091e-02,  4.6879e-02,  ...,  4.9430e-02,\n",
       "            4.8796e-02,  4.7187e-02],\n",
       "          [ 3.5727e-02,  3.7901e-02,  4.0362e-02,  ...,  3.4831e-02,\n",
       "            3.1521e-02,  3.3556e-02]],\n",
       " \n",
       "         [[-5.1383e-02, -5.6293e-02, -5.9644e-02,  ..., -5.7011e-02,\n",
       "           -5.6926e-02, -5.9111e-02],\n",
       "          [-1.0403e-02, -1.0031e-02, -9.2605e-03,  ...,  1.8100e-03,\n",
       "            5.2113e-03, -2.6451e-03],\n",
       "          [ 3.0505e-02,  2.9349e-02,  2.6603e-02,  ...,  3.5301e-02,\n",
       "            3.8561e-02,  3.3970e-02],\n",
       "          ...,\n",
       "          [-5.1762e-02, -5.0154e-02, -4.8962e-02,  ..., -5.3540e-02,\n",
       "           -5.0889e-02, -5.0197e-02],\n",
       "          [ 4.7605e-02,  5.1861e-02,  5.4499e-02,  ...,  4.9418e-02,\n",
       "            5.2202e-02,  4.6013e-02],\n",
       "          [ 2.8676e-02,  2.7417e-02,  2.6791e-02,  ...,  2.4694e-02,\n",
       "            2.6668e-02,  2.9999e-02]],\n",
       " \n",
       "         [[-5.4797e-02, -6.0154e-02, -6.4090e-02,  ..., -5.7918e-02,\n",
       "           -6.0723e-02, -5.5050e-02],\n",
       "          [-1.8703e-02, -1.7260e-02, -1.5553e-02,  ..., -9.0638e-04,\n",
       "           -3.7611e-03, -6.5451e-03],\n",
       "          [ 8.8547e-03,  1.0010e-02,  9.9122e-03,  ...,  1.5414e-02,\n",
       "            1.8574e-02,  1.7085e-02],\n",
       "          ...,\n",
       "          [-5.9420e-02, -5.7654e-02, -5.5436e-02,  ..., -5.5916e-02,\n",
       "           -5.6818e-02, -6.1356e-02],\n",
       "          [ 2.6549e-02,  2.9422e-02,  3.2724e-02,  ...,  2.8597e-02,\n",
       "            2.9226e-02,  2.9814e-02],\n",
       "          [ 2.6310e-02,  2.6002e-02,  2.6295e-02,  ...,  2.0906e-02,\n",
       "            2.1693e-02,  2.2184e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-6.2265e-02, -6.4439e-02, -6.5632e-02,  ..., -6.7155e-02,\n",
       "           -6.9018e-02, -6.5279e-02],\n",
       "          [-2.7557e-02, -2.5838e-02, -2.3914e-02,  ..., -1.1400e-02,\n",
       "           -1.7714e-02, -1.5138e-02],\n",
       "          [ 2.5328e-02,  2.9259e-02,  3.1878e-02,  ...,  3.4856e-02,\n",
       "            3.9662e-02,  3.4708e-02],\n",
       "          ...,\n",
       "          [-6.9258e-02, -6.9583e-02, -6.9693e-02,  ..., -7.0447e-02,\n",
       "           -6.6482e-02, -6.7669e-02],\n",
       "          [ 3.1799e-02,  3.2876e-02,  3.4420e-02,  ...,  3.6943e-02,\n",
       "            3.0294e-02,  3.8821e-02],\n",
       "          [ 3.5513e-02,  3.2590e-02,  3.0462e-02,  ...,  3.0273e-02,\n",
       "            2.7855e-02,  3.0676e-02]],\n",
       " \n",
       "         [[-4.7367e-02, -5.0148e-02, -5.2055e-02,  ..., -4.7518e-02,\n",
       "           -4.7339e-02, -5.3148e-02],\n",
       "          [-2.0310e-02, -1.8733e-02, -1.6761e-02,  ..., -7.5206e-03,\n",
       "           -5.4914e-03, -4.6342e-03],\n",
       "          [ 5.1617e-03,  7.1797e-03,  7.9610e-03,  ...,  1.7879e-02,\n",
       "            2.1691e-02,  9.4624e-03],\n",
       "          ...,\n",
       "          [-6.1529e-02, -6.0211e-02, -5.8687e-02,  ..., -6.2743e-02,\n",
       "           -6.3686e-02, -6.3758e-02],\n",
       "          [ 2.4118e-02,  2.7152e-02,  2.9300e-02,  ...,  2.8047e-02,\n",
       "            2.6935e-02,  2.5863e-02],\n",
       "          [ 2.0537e-02,  1.7300e-02,  1.4703e-02,  ...,  1.5893e-02,\n",
       "            1.3538e-02,  1.4340e-02]],\n",
       " \n",
       "         [[-9.2251e-02, -9.4002e-02, -9.5262e-02,  ..., -9.9422e-02,\n",
       "           -9.4105e-02, -9.6754e-02],\n",
       "          [-3.5038e-02, -3.1956e-02, -2.9154e-02,  ..., -2.1390e-02,\n",
       "           -1.9721e-02, -1.9888e-02],\n",
       "          [ 2.2595e-02,  2.8344e-02,  3.2382e-02,  ...,  3.6560e-02,\n",
       "            3.4000e-02,  3.7588e-02],\n",
       "          ...,\n",
       "          [-8.9551e-02, -8.8527e-02, -8.6874e-02,  ..., -9.2019e-02,\n",
       "           -8.8322e-02, -8.7183e-02],\n",
       "          [ 3.5997e-03,  4.1940e-03,  5.4540e-03,  ..., -8.2262e-05,\n",
       "            4.7310e-03,  5.0176e-03],\n",
       "          [ 3.2568e-02,  3.0935e-02,  2.9628e-02,  ...,  3.6951e-02,\n",
       "            3.4214e-02,  3.1886e-02]]], device='cuda:0'),\n",
       " tensor([[[-8.5154e-04,  1.8140e-01, -2.4656e-01,  ..., -2.4208e+00,\n",
       "           -7.7708e-01,  1.3771e-01],\n",
       "          [-7.6224e-01, -4.8153e-01, -2.8371e-01,  ...,  5.6263e-01,\n",
       "           -4.6842e-01, -2.0108e-01],\n",
       "          [ 4.6080e-01, -1.2984e-01, -1.8394e-03,  ...,  1.0661e+00,\n",
       "            9.5269e-01,  4.5026e-01],\n",
       "          ...,\n",
       "          [ 1.1154e+00,  6.6308e-01, -6.5425e-01,  ..., -7.9407e-01,\n",
       "           -3.9553e-01,  1.6268e-01],\n",
       "          [-1.1972e-01,  3.3988e-01, -9.8537e-01,  ...,  9.1637e-01,\n",
       "           -9.4060e-03,  3.5947e-01],\n",
       "          [-7.5476e-01,  8.4712e-01, -1.5883e+00,  ...,  1.8859e+00,\n",
       "            1.8622e-01, -4.5765e-01]],\n",
       " \n",
       "         [[ 3.3440e-01,  4.8692e-02,  4.8583e-02,  ..., -2.9679e-01,\n",
       "            4.2354e-01,  6.5152e-01],\n",
       "          [-1.5324e-02, -4.4278e-02,  6.7997e-01,  ..., -9.8024e-01,\n",
       "           -1.5881e-01, -8.9621e-02],\n",
       "          [-1.0245e+00, -8.2195e-01, -4.5847e-01,  ..., -1.1834e+00,\n",
       "            2.6896e-02,  1.0039e+00],\n",
       "          ...,\n",
       "          [-1.3690e-01,  6.1575e-01, -2.6878e-01,  ..., -3.7227e-01,\n",
       "            5.4217e-01, -2.0590e-01],\n",
       "          [ 3.5480e-01, -1.7872e+00, -3.7204e-01,  ...,  6.8737e-01,\n",
       "            5.0106e-01, -8.4990e-01],\n",
       "          [ 2.7889e-01,  4.9267e-01, -5.3257e-02,  ..., -2.4928e-01,\n",
       "            2.5332e-01, -1.2106e-01]],\n",
       " \n",
       "         [[-1.1631e+00,  1.1529e-01, -1.2513e+00,  ...,  1.8797e-01,\n",
       "           -7.7955e-01,  7.7526e-01],\n",
       "          [-1.1140e-01,  1.4073e+00, -9.5215e-01,  ..., -1.3020e-02,\n",
       "           -2.0450e+00,  4.8689e-01],\n",
       "          [ 5.3310e-01,  3.8037e-01, -4.9502e-01,  ..., -9.8578e-01,\n",
       "           -1.2592e+00,  1.4522e-01],\n",
       "          ...,\n",
       "          [ 1.4844e+00,  1.2087e-01, -1.1117e+00,  ..., -9.3474e-01,\n",
       "           -8.3738e-01,  1.4827e-01],\n",
       "          [ 2.9497e-01, -1.7113e-01, -1.1118e+00,  ...,  1.5315e-01,\n",
       "            6.5198e-02, -5.9989e-01],\n",
       "          [ 5.5161e-01,  3.6193e-01, -4.1901e-02,  ..., -8.0698e-01,\n",
       "            4.6484e-01, -5.4583e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.9536e-01,  2.0812e-02, -1.0146e+00,  ...,  1.3588e-01,\n",
       "            1.1704e+00,  2.0665e-01],\n",
       "          [-1.1170e-01, -7.5633e-01, -1.2277e+00,  ...,  2.1582e+00,\n",
       "            7.0122e-01,  2.0939e-01],\n",
       "          [-6.7077e-01,  3.6719e-01, -6.5610e-01,  ...,  3.0133e-01,\n",
       "           -1.0934e+00,  3.8009e-01],\n",
       "          ...,\n",
       "          [-1.1023e-01, -4.3526e-01,  1.6744e-02,  ...,  8.3727e-01,\n",
       "            3.7810e-01,  1.5006e-01],\n",
       "          [ 2.1676e-01,  1.1408e+00, -8.0086e-01,  ..., -1.9832e+00,\n",
       "           -9.1198e-01,  4.2430e-01],\n",
       "          [ 1.8321e-01, -3.2338e-01, -7.5046e-02,  ...,  6.0900e-01,\n",
       "            4.2761e-01,  7.2916e-02]],\n",
       " \n",
       "         [[ 6.0533e-01, -4.2438e-01,  2.0648e-01,  ...,  1.7854e+00,\n",
       "           -1.1070e-01,  9.1720e-01],\n",
       "          [-7.5026e-02,  8.4007e-01, -3.7018e-01,  ..., -3.4793e-01,\n",
       "           -8.2730e-01,  8.4535e-01],\n",
       "          [ 7.7409e-01,  8.9269e-01, -4.4572e-01,  ..., -4.5605e-01,\n",
       "           -1.5192e+00, -3.1991e-01],\n",
       "          ...,\n",
       "          [ 9.5481e-01, -1.2675e-01, -2.4892e-01,  ...,  2.8515e-01,\n",
       "            1.7325e+00,  1.5908e-01],\n",
       "          [-1.1296e-02,  1.3700e+00,  4.9536e-01,  ...,  4.0222e-01,\n",
       "            4.8601e-01, -3.5388e-01],\n",
       "          [-9.8050e-02, -1.7564e-01, -1.0290e+00,  ..., -9.2676e-01,\n",
       "           -1.3316e-01, -4.1311e-01]],\n",
       " \n",
       "         [[-8.8326e-01, -3.8398e-01,  1.3756e-01,  ..., -8.2997e-01,\n",
       "            1.5373e+00, -2.6864e-01],\n",
       "          [ 1.0552e+00, -1.0544e+00, -1.1279e+00,  ...,  3.0621e-01,\n",
       "           -4.8461e-01,  2.3491e-01],\n",
       "          [-5.0773e-01,  4.7767e-01, -5.0560e-01,  ..., -6.2465e-01,\n",
       "            5.3823e-01, -5.9743e-01],\n",
       "          ...,\n",
       "          [ 7.6623e-01, -4.7411e-01,  7.3550e-01,  ...,  7.8576e-01,\n",
       "            3.6638e-01, -4.5623e-01],\n",
       "          [-4.6869e-01,  5.9432e-01, -2.1474e-01,  ...,  7.0437e-01,\n",
       "            4.3393e-01, -7.3611e-01],\n",
       "          [ 1.7793e+00, -1.0391e+00, -3.3992e-01,  ...,  5.6027e-01,\n",
       "            3.5371e-01, -9.5813e-02]]], device='cuda:0'),\n",
       " tensor([[ 0.0267,  0.0277,  0.0278,  ...,  0.0198,  0.0178,  0.0214],\n",
       "         [ 0.0142,  0.0171,  0.0183,  ...,  0.0066,  0.0055,  0.0045],\n",
       "         [ 0.0222,  0.0238,  0.0248,  ...,  0.0177,  0.0161,  0.0180],\n",
       "         ...,\n",
       "         [ 0.0094,  0.0114,  0.0115,  ...,  0.0044,  0.0029, -0.0020],\n",
       "         [ 0.0144,  0.0152,  0.0147,  ...,  0.0109,  0.0119,  0.0080],\n",
       "         [ 0.0322,  0.0343,  0.0350,  ...,  0.0308,  0.0270,  0.0235]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(33.4727, device='cuda:0')"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(data[1].cuda(), decoder_output[0], decoder_output[1], data[2].cuda(), decoder_output[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6583,  0.1691, -1.4776,  ..., -0.5090,  1.3761, -0.1395],\n",
       "         [ 0.7005,  0.3326,  1.5623,  ...,  0.8281,  0.1491,  0.4642],\n",
       "         [-0.1606,  0.7656,  0.1319,  ...,  0.5466,  0.0921, -1.0205],\n",
       "         ...,\n",
       "         [ 0.1107,  0.0927, -0.6015,  ...,  0.3849, -0.0065, -0.2685],\n",
       "         [ 0.6088,  0.9316, -0.6893,  ...,  0.7076,  0.8728,  0.5165],\n",
       "         [ 0.3180, -0.6657,  0.1599,  ..., -0.1405, -0.8220, -0.7299]],\n",
       "\n",
       "        [[ 0.5274,  0.1623,  0.5567,  ...,  0.5036, -0.9857, -0.7307],\n",
       "         [-0.4895, -1.0142, -0.0366,  ...,  0.2099, -0.3934,  0.0809],\n",
       "         [-0.2767,  0.8930, -0.9992,  ...,  0.4480,  0.1019, -0.8237],\n",
       "         ...,\n",
       "         [-0.4323, -0.0780,  0.5408,  ..., -0.4314, -0.9461, -0.1175],\n",
       "         [ 0.0771, -0.9793,  0.8653,  ..., -0.1375,  0.9183, -0.7513],\n",
       "         [ 0.1413,  0.0856,  1.5972,  ...,  0.3891, -1.1657, -0.0515]],\n",
       "\n",
       "        [[ 0.1181, -0.2188, -0.8945,  ..., -1.2362,  1.1403, -0.0732],\n",
       "         [ 0.8851,  0.0421, -0.0420,  ...,  0.1683,  1.2347, -0.7623],\n",
       "         [ 1.2375, -0.3960, -0.9537,  ..., -0.1728,  0.5442,  0.4548],\n",
       "         ...,\n",
       "         [ 0.2909,  0.3479,  1.0883,  ..., -0.6221,  0.6736, -0.9812],\n",
       "         [-1.3281, -0.4742, -0.6072,  ...,  0.4260,  0.8588,  0.3709],\n",
       "         [-0.2776, -0.2142,  0.9055,  ..., -0.4031,  1.3266,  0.0643]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.2948, -0.1010, -1.1101,  ..., -0.2660,  0.3113, -0.2971],\n",
       "         [-0.7428, -1.1508,  0.3503,  ..., -0.1983, -0.4515,  0.3273],\n",
       "         [ 1.3690, -0.9844, -0.7261,  ...,  0.2948,  0.2214,  0.7807],\n",
       "         ...,\n",
       "         [ 0.7825, -1.4811, -1.1958,  ...,  0.0480,  0.1725,  0.3162],\n",
       "         [ 0.1160,  0.9674,  0.3149,  ...,  0.6101,  1.0465,  1.2281],\n",
       "         [ 1.7938,  0.6494,  1.2353,  ..., -1.4282,  0.1052, -0.0510]],\n",
       "\n",
       "        [[-0.7698, -0.2600, -2.1533,  ...,  0.7826, -0.3691, -0.6289],\n",
       "         [-0.0232,  0.9298, -0.2962,  ..., -0.9563, -0.8064,  0.2245],\n",
       "         [-0.9104, -0.1009,  1.1673,  ...,  0.7404,  1.3091, -0.3594],\n",
       "         ...,\n",
       "         [ 1.3765, -0.3887, -0.0500,  ...,  0.3834,  0.1010,  0.7176],\n",
       "         [-0.4232, -2.1097,  1.4517,  ..., -0.5311, -0.9078,  0.1412],\n",
       "         [ 0.0339, -1.3725,  0.3109,  ..., -2.4335, -0.3294,  0.2091]],\n",
       "\n",
       "        [[-0.4250, -0.2356, -1.0736,  ..., -0.7526,  0.1165, -0.0144],\n",
       "         [-1.2108, -0.0668,  0.9917,  ...,  0.3322,  0.5354,  0.3287],\n",
       "         [ 0.9053,  0.0183,  0.2625,  ..., -0.5457, -0.1057, -0.9005],\n",
       "         ...,\n",
       "         [ 0.7992, -0.0793, -0.3000,  ..., -0.1377, -0.3887,  0.9783],\n",
       "         [ 0.0141,  0.0992, -0.6380,  ...,  0.5193, -0.2570, -0.2823],\n",
       "         [ 1.7133,  1.0416,  0.0949,  ..., -0.5665,  0.7578,  0.6051]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.5660e-02, -3.9718e-02, -4.2668e-02,  ..., -4.0272e-02,\n",
       "          -3.9391e-02, -4.2186e-02],\n",
       "         [-2.5500e-02, -2.3928e-02, -2.2079e-02,  ..., -1.1441e-02,\n",
       "          -9.4777e-03, -9.4681e-03],\n",
       "         [ 1.2907e-02,  1.5863e-02,  1.8014e-02,  ...,  2.2873e-02,\n",
       "           2.8016e-02,  2.1616e-02],\n",
       "         ...,\n",
       "         [-6.1557e-02, -6.2468e-02, -6.2598e-02,  ..., -6.2267e-02,\n",
       "          -6.1395e-02, -6.1663e-02],\n",
       "         [ 4.2395e-02,  4.2034e-02,  4.1709e-02,  ...,  4.5554e-02,\n",
       "           4.6440e-02,  4.7393e-02],\n",
       "         [ 2.6135e-02,  2.8165e-02,  3.0416e-02,  ...,  2.4501e-02,\n",
       "           2.3975e-02,  2.2832e-02]],\n",
       "\n",
       "        [[-7.2453e-02, -7.7478e-02, -8.0861e-02,  ..., -7.7089e-02,\n",
       "          -7.7956e-02, -7.7716e-02],\n",
       "         [-2.3804e-02, -2.3456e-02, -2.2719e-02,  ..., -1.2019e-02,\n",
       "          -9.2107e-03, -7.5158e-03],\n",
       "         [ 2.5730e-02,  2.5117e-02,  2.3034e-02,  ...,  3.4300e-02,\n",
       "           3.3576e-02,  3.5578e-02],\n",
       "         ...,\n",
       "         [-7.4056e-02, -7.2287e-02, -7.0978e-02,  ..., -7.8641e-02,\n",
       "          -7.3753e-02, -7.3213e-02],\n",
       "         [ 2.3545e-02,  2.8333e-02,  3.1393e-02,  ...,  2.5279e-02,\n",
       "           2.4347e-02,  2.7195e-02],\n",
       "         [ 2.0033e-02,  1.8973e-02,  1.8661e-02,  ...,  1.8152e-02,\n",
       "           1.2693e-02,  1.9959e-02]],\n",
       "\n",
       "        [[-5.3290e-02, -5.8911e-02, -6.3152e-02,  ..., -5.9911e-02,\n",
       "          -5.7783e-02, -6.0942e-02],\n",
       "         [-1.5903e-03,  3.2175e-05,  1.8252e-03,  ...,  8.9619e-03,\n",
       "           1.3698e-02,  1.2390e-02],\n",
       "         [ 3.4929e-03,  4.6509e-03,  4.4337e-03,  ...,  1.0175e-02,\n",
       "           5.9091e-03,  7.4483e-03],\n",
       "         ...,\n",
       "         [-5.4963e-02, -5.3339e-02, -5.1228e-02,  ..., -5.5711e-02,\n",
       "          -5.4942e-02, -5.7027e-02],\n",
       "         [ 2.3385e-02,  2.6265e-02,  2.9741e-02,  ...,  2.7531e-02,\n",
       "           2.4026e-02,  2.6912e-02],\n",
       "         [ 2.8449e-02,  2.8142e-02,  2.8395e-02,  ...,  2.2817e-02,\n",
       "           2.3548e-02,  2.5420e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.0655e-02, -7.2759e-02, -7.3730e-02,  ..., -7.4506e-02,\n",
       "          -7.4854e-02, -7.5617e-02],\n",
       "         [-3.4293e-02, -3.2148e-02, -2.9807e-02,  ..., -2.2121e-02,\n",
       "          -2.1974e-02, -2.1118e-02],\n",
       "         [ 1.8946e-02,  2.2561e-02,  2.4783e-02,  ...,  2.3503e-02,\n",
       "           1.9796e-02,  2.3503e-02],\n",
       "         ...,\n",
       "         [-5.6229e-02, -5.6478e-02, -5.6534e-02,  ..., -5.7363e-02,\n",
       "          -5.7145e-02, -5.4724e-02],\n",
       "         [ 4.1267e-02,  4.2803e-02,  4.4885e-02,  ...,  4.4213e-02,\n",
       "           4.6017e-02,  4.4284e-02],\n",
       "         [ 2.4352e-02,  2.1726e-02,  1.9869e-02,  ...,  1.7191e-02,\n",
       "           2.4010e-02,  2.1433e-02]],\n",
       "\n",
       "        [[-5.4828e-02, -5.7574e-02, -5.9371e-02,  ..., -6.3925e-02,\n",
       "          -6.0727e-02, -6.0386e-02],\n",
       "         [-2.6813e-02, -2.5363e-02, -2.3721e-02,  ..., -1.1368e-02,\n",
       "          -1.5692e-02, -1.0309e-02],\n",
       "         [ 1.4047e-02,  1.6138e-02,  1.7044e-02,  ...,  1.7776e-02,\n",
       "           2.1139e-02,  2.0620e-02],\n",
       "         ...,\n",
       "         [-6.6457e-02, -6.5246e-02, -6.3764e-02,  ..., -6.6283e-02,\n",
       "          -6.4716e-02, -6.2595e-02],\n",
       "         [ 1.5247e-02,  1.8197e-02,  2.0278e-02,  ...,  1.6958e-02,\n",
       "           1.3684e-02,  1.7637e-02],\n",
       "         [ 2.5405e-02,  2.2340e-02,  1.9943e-02,  ...,  2.0527e-02,\n",
       "           2.3078e-02,  2.1012e-02]],\n",
       "\n",
       "        [[-8.8275e-02, -9.0506e-02, -9.2180e-02,  ..., -9.8449e-02,\n",
       "          -9.3863e-02, -9.3918e-02],\n",
       "         [-3.1524e-02, -2.8768e-02, -2.6238e-02,  ..., -1.8265e-02,\n",
       "          -1.7799e-02, -2.0534e-02],\n",
       "         [ 4.5431e-02,  5.0805e-02,  5.4427e-02,  ...,  5.2678e-02,\n",
       "           5.4158e-02,  6.2712e-02],\n",
       "         ...,\n",
       "         [-6.1186e-02, -6.0373e-02, -5.8878e-02,  ..., -5.9268e-02,\n",
       "          -6.5302e-02, -6.3860e-02],\n",
       "         [ 2.5100e-02,  2.5885e-02,  2.7101e-02,  ...,  2.6103e-02,\n",
       "           3.0080e-02,  2.9462e-02],\n",
       "         [ 5.2906e-02,  5.0995e-02,  4.9416e-02,  ...,  5.2786e-02,\n",
       "           5.0383e-02,  5.3020e-02]]], device='cuda:0')"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1500])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 80, 558]), torch.Size([32, 80, 558]), torch.Size([32, 558]))"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output[0].shape, decoder_output[1].shape, decoder_output[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 80, 558])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 80]), torch.Size([32, 1, 512]), torch.Size([32, 84, 1]))"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output[0].shape, decoder_output[1].shape, decoder_output[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 80, 558])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_input_decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 84, 512])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 84, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([558, 32, 256])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention(mel_unit[0], temp_input_encoder)[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 558])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_input_decoder[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84, 512])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 558, 80])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_input_decoder.transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Identity()(torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = decoder(temp_input_decoder, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 558, 128])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 84, 128])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 84, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 558, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(data[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 80, 558])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 80, 558])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_input = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = dataset.preprocessor.vocab.shape[0]+1\n",
    "model = Tacotron2(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 84, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(temp_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 80, 558])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 558])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.characters_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_input.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
